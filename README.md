# Adversarial-Attack-on-MNIST-Data

Deep learning models are vulnerable to well crafted and knowingly framed *adversarial attaks*. These attacks are created by adding small amount of imperceptible perturbation to the input of deep learning models that leads to misclassify the output.
